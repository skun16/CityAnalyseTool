{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82e5b8d3-f39c-42b9-bcc0-fe8d08acffd9",
   "metadata": {},
   "source": [
    "## 链家小区均价"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be015d5c-efdb-4460-a7db-41e6024e910a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 爬取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "839395fa-2b75-40f9-ae29-c51c3dd09ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共328条待爬取，共11页待爬取\n",
      " 第11页已爬取数据已保存,共325条\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "district = 'huangpugz'\n",
    "url = 'https://gz.lianjia.com/xiaoqu/{}/'.format(district)\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()\n",
    "\n",
    "\n",
    "# 每页30条\n",
    "r = requests.get(url+'pg1',headers={\"User-Agent\":ua.random})\n",
    "lj_html = r.text\n",
    "lj_soup =BeautifulSoup(lj_html,'html.parser')\n",
    "house_counts = int(re.findall(r\"\\d+\\.?\\d*\",lj_soup.find('h2', class_='total fl').get_text())[0])\n",
    "print('共{}条待爬取，共{}页待爬取'.format(house_counts,int(int(house_counts)/30)+1))\n",
    "\n",
    "community_infos = []\n",
    "\n",
    "for i in range(1,int(house_counts/30)+2):\n",
    "    r = requests.get(url+'pg{}'.format(i),headers={\"User-Agent\":ua.random})\n",
    "    lj_html = r.text\n",
    "    lj_soup =BeautifulSoup(lj_html,'html.parser')\n",
    "    communities = lj_soup.find_all('li', class_='clear xiaoquListItem')\n",
    "    for community in communities:\n",
    "        # print(type(community))\n",
    "        # print(community)\n",
    "        #print(community.find('div', class_ = 'totalPrice').get_text().strip())\n",
    "        name = community.find('div', class_ = 'title').get_text().strip()\n",
    "        price = community.find('div', class_ = 'totalPrice').get_text().strip()\n",
    "        sales = community.find('a', title = re.compile('网签')).get_text().strip()\n",
    "        hire = community.find('a', title = re.compile('租房')).get_text().strip()\n",
    "        district = community.find('a', class_ = 'district').get_text().strip()\n",
    "        bizcircle = community.find('a', class_ = 'bizcircle').get_text().strip()\n",
    "        # time = re.compile(community)\n",
    "        counts = community.find('a', class_ = 'totalSellCount').get_text().strip()\n",
    "        community_info = [name,price,sales,hire, district,bizcircle,counts]\n",
    "        # print(community_info)\n",
    "        community_infos.append(community_info)\n",
    "    time.sleep(1)\n",
    "    print('\\r','第{}页已爬取'.format(i), end='',flush=True)\n",
    " \n",
    "# print(community_infos) \n",
    "name = ['小区名', '单价','90天成交量', '出租数', '所在区','所在街道', '在售数量']\n",
    "community_data = pd.DataFrame(columns=name,data=community_infos)\n",
    "community_data.to_csv('{}.csv'.format(district),encoding='utf-8-sig')\n",
    "print('数据已保存,共{}条'.format(len(community_data)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d08233bb-f460-4c34-bea5-456cf10806ff",
   "metadata": {},
   "source": [
    "### 空间化"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d06d55a-3ffc-4c65-9bb5-8d8fea76dd5f",
   "metadata": {},
   "source": [
    "##### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da573e60-baf5-4f37-bf84-63a3a93d3df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小区名已读取\n",
      " 7417/7596,增城时代廊桥已爬取号已爬取6号)已爬取爬取"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'community_infos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 67>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m项爬取失败\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i))\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# print(communities_info)\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m community_location \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39m\u001b[43mcommunity_infos\u001b[49m)\n\u001b[0;32m     68\u001b[0m community_location\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation0711.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m已储存\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'community_infos' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()\n",
    "\n",
    "\n",
    "ak = 'yWaZutEsNSwv6AQkMiFipuRMzIQtHC8X'\n",
    "url = 'https://api.map.baidu.com/place/v2/search?'\n",
    "\n",
    "community_price = pd.read_csv('D:\\Lianjia\\链家小区均价.csv',encoding='gbk',sep=',')\n",
    "community_price\n",
    "community_name = community_price.iloc[:,1]\n",
    "community_name_list = np.array(community_name).tolist()\n",
    "print('小区名已读取')\n",
    "# communities = community_name_list\n",
    "\n",
    "# 读取小区名\n",
    "communities_info = []\n",
    "i = 0\n",
    "failed = 0\n",
    "completed = 0\n",
    "for community in community_name_list:   \n",
    "    # query = community\n",
    "    payload = {'query':community,'region':'广州','output':'json','ak':ak,'scope':'2','city_limit':'true'}\n",
    "    r = requests.get(url,params=payload,headers={\"User-Agent\":ua.random})\n",
    "    # print(r.url)\n",
    "    # print(r.text)\n",
    "    # print('\\r','网页已获取', end='',flush=True)\n",
    "    data = r.json()\n",
    "    # status = data['status']\n",
    "    try:\n",
    "        name = data['results'][0]['name']\n",
    "        lat = data['results'][0]['location']['lat']\n",
    "        lng = data['results'][0]['location']['lng']\n",
    "        address = data['results'][0]['address']\n",
    "        tag = data['results'][0]['detail_info']['tag']\n",
    "        community_info = [community, name, lat, lng, address, tag]\n",
    "        communities_info.append(community_info)\n",
    "        completed += 1\n",
    "        # print('已获取')\n",
    "        print('\\r','{}/{},{}已爬取'.format(completed,len(community_name_list),community).ljust(20), end='',flush=True)\n",
    "    except:\n",
    "        i += 1\n",
    "        continue\n",
    "        print('{}项爬取失败'.format(i))\n",
    "    \n",
    "# print(communities_info)\n",
    "community_location = pd.DataFrame(data=communities_info)\n",
    "community_location.to_csv('location0711.csv',encoding='utf-8-sig')\n",
    "print('已储存')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "292db660-27f1-417a-8f77-85e6c1dcaf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已储存\n"
     ]
    }
   ],
   "source": [
    "community_location = pd.DataFrame(data=communities_info)\n",
    "community_location.to_csv('location0711.csv',encoding='utf-8-sig')\n",
    "print('已储存')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7df8413-8673-4bed-8245-1ea2841745a9",
   "metadata": {},
   "source": [
    "### 坐标转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "976db80d-d7a4-49b1-8109-746f81fc9bfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd0 in position 0: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 99>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsage: type must be in one of g2b, b2g, w2g, g2w, b2w, w2b\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m         sys\u001b[38;5;241m.\u001b[39mexit()\n\u001b[1;32m---> 99\u001b[0m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(INPUT, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m     30\u001b[0m     input_file_reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(input_file)\n\u001b[1;32m---> 31\u001b[0m     headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_file_reader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     lng_index, lat_index \u001b[38;5;241m=\u001b[39m get_lng_lat_index(headers)\n\u001b[0;32m     33\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\encodings\\utf_8_sig.py:69\u001b[0m, in \u001b[0;36mIncrementalDecoder._buffer_decode\u001b[1;34m(self, input, errors, final)\u001b[0m\n\u001b[0;32m     66\u001b[0m             (output, consumed) \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m     67\u001b[0m                codecs\u001b[38;5;241m.\u001b[39mutf_8_decode(\u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m3\u001b[39m:], errors, final)\n\u001b[0;32m     68\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m (output, consumed\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutf_8_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd0 in position 0: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import argparse\n",
    "from coordTransform_utils import gcj02_to_bd09\n",
    "from coordTransform_utils import bd09_to_gcj02\n",
    "from coordTransform_utils import wgs84_to_gcj02\n",
    "from coordTransform_utils import gcj02_to_wgs84\n",
    "from coordTransform_utils import bd09_to_wgs84\n",
    "from coordTransform_utils import wgs84_to_bd09\n",
    "\n",
    "# Configuration\n",
    "# Input file name\n",
    "INPUT = 'D:\\Lianjia\\链家小区均价和位置_清洗.xls'\n",
    "# Output file name\n",
    "OUTPUT = 'D:\\Lianjia\\链家小区均价和位置_坐标.xls'\n",
    "# Convert type: g2b, b2g, w2g, g2w, b2w, w2b\n",
    "TYPE = 'b2w'\n",
    "# lng column name\n",
    "LNG_COLUMN = 'longitude'\n",
    "# lat column name\n",
    "LAT_COLUMN = 'latitude'\n",
    "# Skip invalid row\n",
    "SKIP_INVALID_ROW = False\n",
    "\n",
    "def convert():\n",
    "    with open(INPUT, 'r', encoding='gbk') as input_file:\n",
    "        input_file_reader = csv.reader(input_file)\n",
    "        headers = next(input_file_reader)\n",
    "        lng_index, lat_index = get_lng_lat_index(headers)\n",
    "        results = []\n",
    "\n",
    "        for index, row in enumerate(input_file_reader):\n",
    "            result = []\n",
    "            try:\n",
    "                result = convert_by_type(float(row[lng_index]), float(row[lat_index]), TYPE)\n",
    "            except ValueError:\n",
    "                # Deal with ValueError(invalid lng or lat)\n",
    "                # print(index + 2, row[lng_index], row[lat_index]) # '+ 2' is due to zero-based index and first row is header\n",
    "                result = row[lng_index], row[lat_index]\n",
    "            results.append(result)\n",
    "\n",
    "    with open(OUTPUT, 'w') as output_file:\n",
    "        output_file_writer = csv.writer(output_file, lineterminator='\\n')\n",
    "\n",
    "        with open(INPUT, 'r') as input_file:\n",
    "            input_file_reader = csv.reader(input_file)\n",
    "            headers = next(input_file_reader)\n",
    "            lng_index, lat_index = get_lng_lat_index(headers)\n",
    "\n",
    "            output_file_writer.writerow(headers)\n",
    "            for index, row in enumerate(input_file_reader):\n",
    "                row[lng_index] = results[index][0]\n",
    "                row[lat_index] = results[index][1]\n",
    "                if type(row[lng_index]) is not float or type(row[lat_index]) is not float:\n",
    "                    # Data is invalid\n",
    "                    if SKIP_INVALID_ROW:\n",
    "                        # Skip invalid row\n",
    "                        pass\n",
    "                    else:\n",
    "                        # Reserve invalid row\n",
    "                        output_file_writer.writerow(row)\n",
    "                else:\n",
    "                    # Data is valid\n",
    "                    output_file_writer.writerow(row)\n",
    "\n",
    "def get_lng_lat_index(headers):\n",
    "    try:\n",
    "        if LNG_COLUMN == '' and LAT_COLUMN == '':\n",
    "            return [headers.index('lng'), headers.index('lat')]\n",
    "        else:\n",
    "            return [headers.index(LNG_COLUMN), headers.index(LAT_COLUMN)]\n",
    "    except ValueError as error:\n",
    "        print('Error: ' + str(error).split('is', 1)[0] + 'is missing from csv header. Or use -n or -a to specify custom column name for lng or lat.')\n",
    "        sys.exit()\n",
    "\n",
    "def convert_by_type(lng, lat, type):\n",
    "    if type == 'g2b':\n",
    "        return gcj02_to_bd09(lng, lat)\n",
    "    elif type == 'b2g':\n",
    "        return bd09_to_gcj02(lng, lat)\n",
    "    elif type == 'w2g':\n",
    "        return wgs84_to_gcj02(lng, lat)\n",
    "    elif type == 'g2w':\n",
    "        return gcj02_to_wgs84(lng, lat)\n",
    "    elif type == 'b2w':\n",
    "        return bd09_to_wgs84(lng, lat)\n",
    "    elif type == 'w2b':\n",
    "        return wgs84_to_bd09(lng, lat)\n",
    "    else:\n",
    "        print('Usage: type must be in one of g2b, b2g, w2g, g2w, b2w, w2b')\n",
    "        sys.exit()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "convert()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3b41f2a-b249-4766-a155-20f8f17a2336",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded947a2-17f9-461c-a550-e345a08307f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d8d71-713e-420d-a8b7-ecf885eff02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1304892f",
   "metadata": {},
   "source": [
    "# 链家房价0612 全信息爬取"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45bbb256",
   "metadata": {},
   "source": [
    "#### 爬取所有街道的URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c38ffd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/xiaoqu/cencun/', '/xiaoqu/changxing1/', '/xiaoqu/chebei/', '/xiaoqu/dashadi/', '/xiaoqu/dongfengdong/', '/xiaoqu/dongpu/', '/xiaoqu/ershadao/', '/xiaoqu/gaotang/', '/xiaoqu/huajingxincheng/', '/xiaoqu/huangcun/', '/xiaoqu/huanghuagang/', '/xiaoqu/huangpuqufu/', '/xiaoqu/huanshidong/', '/xiaoqu/huijingxincheng/', '/xiaoqu/jinrongcheng1/', '/xiaoqu/kexuecheng/', '/xiaoqu/linhe/', '/xiaoqu/longdong/', '/xiaoqu/longkoudong/', '/xiaoqu/longkouxi/', '/xiaoqu/meihuayuan/', '/xiaoqu/shahe1/', '/xiaoqu/shataibei/', '/xiaoqu/shatainan/', '/xiaoqu/shipai1/', '/xiaoqu/shuiyin/', '/xiaoqu/tangxia1/', '/xiaoqu/tianhegongyuan/', '/xiaoqu/tianhekeyunzhan/', '/xiaoqu/tianhenan/', '/xiaoqu/tianrunlu/', '/xiaoqu/tiyuzhongxin/', '/xiaoqu/wushan/', '/xiaoqu/wuyangxincheng/', '/xiaoqu/yangji/', '/xiaoqu/yantang/', '/xiaoqu/yuancun/', '/xiaoqu/yueken/', '/xiaoqu/yuzhu/', '/xiaoqu/zhihuicheng/', '/xiaoqu/zhujiangxinchengdong/', '/xiaoqu/zhujiangxinchengxi/', '/xiaoqu/zhujiangxinchengzhong/']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "district_list = ['tianhe','yuexiu','liwan','haizhu','panyu','baiyun','huangpuz','conghua','zengcheng','huadou','nansha','nanhai','shunde']\n",
    "\n",
    "for district in district_list:\n",
    "    url = \"https://gz.lianjia.com/xiaoqu/tianhe/\"  # 将URL替换为要爬取的网页地址\n",
    "\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    divs = soup.find_all(\"div\", {\"data-role\": \"ershoufang\"})\n",
    "\n",
    "    if divs:\n",
    "        target_div = divs[0]  # 获取第一个符合条件的div\n",
    "        second_div = target_div.find_all(\"div\")[1]  # 获取第二个div\n",
    "\n",
    "        hrefs = []\n",
    "        for a_tag in second_div.find_all(\"a\"):\n",
    "            href = a_tag.get(\"href\")\n",
    "            if href:\n",
    "                hrefs.append(href)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        print(\"未找到符合条件的div\")\n",
    "print(hrefs)  # 输出提取到的href列表"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35616f2e",
   "metadata": {},
   "source": [
    "#### 爬取所有小区的url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f12a687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs = ['/xiaoqu/cencun/', '/xiaoqu/changxing1/', '/xiaoqu/chebei/', '/xiaoqu/dashadi/', '/xiaoqu/dongfengdong/', '/xiaoqu/dongpu/', '/xiaoqu/ershadao/', '/xiaoqu/gaotang/', '/xiaoqu/huajingxincheng/', '/xiaoqu/huangcun/', '/xiaoqu/huanghuagang/', '/xiaoqu/huangpuqufu/', '/xiaoqu/huanshidong/', '/xiaoqu/huijingxincheng/', '/xiaoqu/jinrongcheng1/', '/xiaoqu/kexuecheng/', '/xiaoqu/linhe/', '/xiaoqu/longdong/', '/xiaoqu/longkoudong/', '/xiaoqu/longkouxi/', '/xiaoqu/meihuayuan/', '/xiaoqu/shahe1/', '/xiaoqu/shataibei/', '/xiaoqu/shatainan/', '/xiaoqu/shipai1/', '/xiaoqu/shuiyin/', '/xiaoqu/tangxia1/', '/xiaoqu/tianhegongyuan/', '/xiaoqu/tianhekeyunzhan/', '/xiaoqu/tianhenan/', '/xiaoqu/tianrunlu/', '/xiaoqu/tiyuzhongxin/', '/xiaoqu/wushan/', '/xiaoqu/wuyangxincheng/', '/xiaoqu/yangji/', '/xiaoqu/yantang/', '/xiaoqu/yuancun/', '/xiaoqu/yueken/', '/xiaoqu/yuzhu/', '/xiaoqu/zhihuicheng/', '/xiaoqu/zhujiangxinchengdong/', '/xiaoqu/zhujiangxinchengxi/', '/xiaoqu/zhujiangxinchengzhong/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4fd32aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 共24条待爬取，共1页待爬取，正在爬取/xiaoqu/zhujiangxinchengzhong/ ['https://gz.lianjia.com/xiaoqu/2111103318219/', 'https://gz.lianjia.com/xiaoqu/219161663066551/', 'https://gz.lianjia.com/xiaoqu/2111103319247/', 'https://gz.lianjia.com/xiaoqu/2111103318979/', 'https://gz.lianjia.com/xiaoqu/2111103318937/', 'https://gz.lianjia.com/xiaoqu/2110343238598891/', 'https://gz.lianjia.com/xiaoqu/2111103831614/', 'https://gz.lianjia.com/xiaoqu/2110819115692726/', 'https://gz.lianjia.com/xiaoqu/2111103318824/', 'https://gz.lianjia.com/xiaoqu/2111103316991/', 'https://gz.lianjia.com/xiaoqu/2111103831496/', 'https://gz.lianjia.com/xiaoqu/2111103318061/', 'https://gz.lianjia.com/xiaoqu/2111103318762/', 'https://gz.lianjia.com/xiaoqu/2111103319096/', 'https://gz.lianjia.com/xiaoqu/2111103318877/', 'https://gz.lianjia.com/xiaoqu/2111103318315/', 'https://gz.lianjia.com/xiaoqu/2120032555860353/', 'https://gz.lianjia.com/xiaoqu/2111103317850/', 'https://gz.lianjia.com/xiaoqu/2120028754189643/', 'https://gz.lianjia.com/xiaoqu/2120037606449325/', 'https://gz.lianjia.com/xiaoqu/2120037568700245/', 'https://gz.lianjia.com/xiaoqu/2120037365189384/']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "\n",
    "community_hrefs = []\n",
    "\n",
    "\n",
    "for district_url in hrefs:\n",
    "\n",
    "    url = 'https://gz.lianjia.com' + district_url\n",
    "    \n",
    "    ua = UserAgent()\n",
    "\n",
    "\n",
    "    # 每页30条\n",
    "    r = requests.get(url+'pg1',headers={\"User-Agent\":ua.random})\n",
    "    lj_html = r.text\n",
    "    lj_soup =BeautifulSoup(lj_html,'html.parser')\n",
    "    house_counts = int(re.findall(r\"\\d+\\.?\\d*\",lj_soup.find('h2', class_='total fl').get_text())[0])\n",
    "    print('\\r','共{}条待爬取，共{}页待爬取，正在爬取{}'.format(house_counts, int(int(house_counts)/30)+1, district_url).ljust(50),end='',flush=True)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        for i in range(1,int(house_counts/30)+2):\n",
    "            r = requests.get(url+'pg{}'.format(i),headers={\"User-Agent\":ua.random})\n",
    "            lj_html = r.text\n",
    "            lj_soup =BeautifulSoup(lj_html,'html.parser')\n",
    "\n",
    "            divs = lj_soup.find_all(\"div\", class_=\"info\")  # 获取所有class为\"info\"的div\n",
    "\n",
    "            for div in divs:\n",
    "                title_div = div.find(\"div\", class_=\"title\")  # 在每个\"info\" div中找到class为\"title\"的div\n",
    "                if title_div:\n",
    "                    a_tag = title_div.find(\"a\")  # 在\"title\" div中找到第一个<a>标签\n",
    "                    if a_tag:\n",
    "                        href = a_tag.get(\"href\")  # 获取<a>标签的href属性值\n",
    "                        if href:\n",
    "                            community_hrefs.append(href)\n",
    "    except Exception:\n",
    "        continue\n",
    "print(community_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11296ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://gz.lianjia.com/xiaoqu/2111103318219/', 'https://gz.lianjia.com/xiaoqu/219161663066551/', 'https://gz.lianjia.com/xiaoqu/2111103319247/', 'https://gz.lianjia.com/xiaoqu/2111103318979/', 'https://gz.lianjia.com/xiaoqu/2111103318937/', 'https://gz.lianjia.com/xiaoqu/2110343238598891/', 'https://gz.lianjia.com/xiaoqu/2111103831614/', 'https://gz.lianjia.com/xiaoqu/2110819115692726/', 'https://gz.lianjia.com/xiaoqu/2111103318824/', 'https://gz.lianjia.com/xiaoqu/2111103316991/', 'https://gz.lianjia.com/xiaoqu/2111103831496/', 'https://gz.lianjia.com/xiaoqu/2111103318061/', 'https://gz.lianjia.com/xiaoqu/2111103318762/', 'https://gz.lianjia.com/xiaoqu/2111103319096/', 'https://gz.lianjia.com/xiaoqu/2111103318877/', 'https://gz.lianjia.com/xiaoqu/2111103318315/', 'https://gz.lianjia.com/xiaoqu/2120032555860353/', 'https://gz.lianjia.com/xiaoqu/2111103317850/', 'https://gz.lianjia.com/xiaoqu/2120028754189643/', 'https://gz.lianjia.com/xiaoqu/2120037606449325/', 'https://gz.lianjia.com/xiaoqu/2120037568700245/', 'https://gz.lianjia.com/xiaoqu/2120037365189384/']\n"
     ]
    }
   ],
   "source": [
    "print(community_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76beebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "for district_url in hrefs:\n",
    "\n",
    "    url = 'https://gz.lianjia.com' + district_url\n",
    "    \n",
    "    ua = UserAgent()\n",
    "\n",
    "\n",
    "    # 每页30条\n",
    "    r = requests.get(url+'pg1',headers={\"User-Agent\":ua.random})\n",
    "    lj_html = r.text\n",
    "    lj_soup =BeautifulSoup(lj_html,'html.parser')\n",
    "    house_counts = int(re.findall(r\"\\d+\\.?\\d*\",lj_soup.find('h2', class_='total fl').get_text())[0])\n",
    "    print('共{}条待爬取，共{}页待爬取'.format(house_counts,int(int(house_counts)/30)+1))\n",
    "\n",
    "    community_infos = []\n",
    "\n",
    "    for i in range(1,int(house_counts/30)+2):\n",
    "        r = requests.get(url+'pg{}'.format(i),headers={\"User-Agent\":ua.random})\n",
    "        lj_html = r.text\n",
    "        lj_soup =BeautifulSoup(lj_html,'html.parser')\n",
    "        communities = lj_soup.find_all('li', class_='clear xiaoquListItem')\n",
    "        for community in communities:\n",
    "            # print(type(community))\n",
    "            # print(community)\n",
    "            #print(community.find('div', class_ = 'totalPrice').get_text().strip())\n",
    "            name = community.find('div', class_ = 'title').get_text().strip()\n",
    "            price = community.find('div', class_ = 'totalPrice').get_text().strip()\n",
    "            sales = community.find('a', title = re.compile('网签')).get_text().strip()\n",
    "            hire = community.find('a', title = re.compile('租房')).get_text().strip()\n",
    "            district = community.find('a', class_ = 'district').get_text().strip()\n",
    "            bizcircle = community.find('a', class_ = 'bizcircle').get_text().strip()\n",
    "            # time = re.compile(community)\n",
    "            counts = community.find('a', class_ = 'totalSellCount').get_text().strip()\n",
    "            community_info = [name,price,sales,hire, district,bizcircle,counts]\n",
    "            # print(community_info)\n",
    "            community_infos.append(community_info)\n",
    "        time.sleep(1)\n",
    "        print('\\r','第{}页已爬取'.format(i), end='',flush=True)\n",
    " \n",
    "# print(community_infos) \n",
    "name = ['小区名', '单价','90天成交量', '出租数', '所在区','所在街道', '在售数量']\n",
    "community_data = pd.DataFrame(columns=name,data=community_infos)\n",
    "community_data.to_csv('{}.csv'.format(district),encoding='utf-8-sig')\n",
    "print('数据已保存,共{}条'.format(len(community_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "district = 'huangpugz'\n",
    "url = 'https://gz.lianjia.com/xiaoqu/{}/'.format(district)\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()\n",
    "\n",
    "\n",
    "# 每页30条\n",
    "r = requests.get(url+'pg1',headers={\"User-Agent\":ua.random})\n",
    "lj_html = r.text\n",
    "lj_soup =BeautifulSoup(lj_html,'html.parser')\n",
    "house_counts = int(re.findall(r\"\\d+\\.?\\d*\",lj_soup.find('h2', class_='total fl').get_text())[0])\n",
    "print('共{}条待爬取，共{}页待爬取'.format(house_counts,int(int(house_counts)/30)+1))\n",
    "\n",
    "community_infos = []\n",
    "\n",
    "for i in range(1,int(house_counts/30)+2):\n",
    "    r = requests.get(url+'pg{}'.format(i),headers={\"User-Agent\":ua.random})\n",
    "    lj_html = r.text\n",
    "    lj_soup =BeautifulSoup(lj_html,'html.parser')\n",
    "    communities = lj_soup.find_all('li', class_='clear xiaoquListItem')\n",
    "    for community in communities:\n",
    "        # print(type(community))\n",
    "        # print(community)\n",
    "        #print(community.find('div', class_ = 'totalPrice').get_text().strip())\n",
    "        name = community.find('div', class_ = 'title').get_text().strip()\n",
    "        price = community.find('div', class_ = 'totalPrice').get_text().strip()\n",
    "        sales = community.find('a', title = re.compile('网签')).get_text().strip()\n",
    "        hire = community.find('a', title = re.compile('租房')).get_text().strip()\n",
    "        district = community.find('a', class_ = 'district').get_text().strip()\n",
    "        bizcircle = community.find('a', class_ = 'bizcircle').get_text().strip()\n",
    "        # time = re.compile(community)\n",
    "        counts = community.find('a', class_ = 'totalSellCount').get_text().strip()\n",
    "        community_info = [name,price,sales,hire, district,bizcircle,counts]\n",
    "        # print(community_info)\n",
    "        community_infos.append(community_info)\n",
    "    time.sleep(1)\n",
    "    print('\\r','第{}页已爬取'.format(i), end='',flush=True)\n",
    " \n",
    "# print(community_infos) \n",
    "name = ['小区名', '单价','90天成交量', '出租数', '所在区','所在街道', '在售数量']\n",
    "community_data = pd.DataFrame(columns=name,data=community_infos)\n",
    "community_data.to_csv('{}.csv'.format(district),encoding='utf-8-sig')\n",
    "print('数据已保存,共{}条'.format(len(community_data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
